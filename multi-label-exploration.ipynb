{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428cbc8d",
   "metadata": {},
   "source": [
    "for the first exploration, i will try to do vectorizer with toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee255d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences \n",
    "docs=[\"the house had a tiny little mouse\", \n",
    "\"the cat saw the mouse\", \n",
    "\"the mouse ran away from the house\", \n",
    "\"the cat finally ate the mouse\", \n",
    "\"the end of the mouse story\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dca840",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "X2 = vectorizer2.fit_transform(docs)\n",
    "vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc474b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a982a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=vectorizer2.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19d014",
   "metadata": {},
   "source": [
    "I grasped the theory a bit about feature extraction in text, next is cleaning i guess? The cleaning consist of : \n",
    "- case folding\n",
    "- data cleaning (removes rt, username, url, punctuation, emoticon)\n",
    "- text normalization (changes non formal words into the formal one)\n",
    "- stemming\n",
    "- stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf86653",
   "metadata": {},
   "source": [
    "# Data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './repo_dataset/id-multi-label-hate-speech-and-abusive-language-detection/re_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26662de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da165e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b3448",
   "metadata": {},
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eea223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f46d58",
   "metadata": {},
   "source": [
    "data cleaning (removes rt, username, url, punctuation, emoticon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "dummy = 'rt user url betul sekali itu 12 !!!'\n",
    "\n",
    "\n",
    "dummy = re.sub(\"rt\",\"\",dummy)\n",
    "dummy = re.sub(\"user\",\"\",dummy)\n",
    "dummy = re.sub(\"url\",\"\",dummy)\n",
    "\n",
    "\n",
    "mod_string = \"\"\n",
    "for elem in dummy:\n",
    "    if elem.isalnum() or elem == ' ':\n",
    "        mod_string += elem\n",
    "dummy = mod_string\n",
    "\n",
    "\n",
    "dummy = \" \".join(dummy.split())\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "\n",
    "    tweet = re.sub(\"rt\",\"\", str(tweet))\n",
    "    tweet = re.sub(\"user\",\"\",str(tweet))\n",
    "    tweet = re.sub(\"url\",\"\",str(tweet))\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', str(tweet)) # punctuation\n",
    "    tweet = re.sub(r\"\\d+\", \"\", str(tweet)) # number\n",
    "    \n",
    "\n",
    "\n",
    "    mod_string = \"\"\n",
    "    for elem in tweet:\n",
    "        if elem.isalnum() or elem == ' ':\n",
    "            mod_string += elem\n",
    "    tweet = mod_string\n",
    "    \n",
    "    res = emoji.emoji_list(tweet)\n",
    "    if len(res) > 0 :\n",
    "        tweet = emoji.replace_emoji(tweet, replace='')\n",
    "\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].map(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36f830",
   "metadata": {},
   "source": [
    "text normalization (changes non formal words into the formal one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = './repo_dataset/id-multi-label-hate-speech-and-abusive-language-detection/new_kamusalay.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48292542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = pd.read_csv(reference, encoding = \"ISO-8859-1\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62590983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = '3x ke kamar menaker mandi'\n",
    "# for item in df_ref[0]:\n",
    "#     print(item)\n",
    "\n",
    "tempDum = dummy.split()\n",
    "\n",
    "# for temp in tempdum:\n",
    "#     for item in df_ref[0]:\n",
    "#         if temp == item:\n",
    "#             print(temp)\n",
    "#             print(item)\n",
    "            \n",
    "for tempInd, tempVal in enumerate(tempDum):\n",
    "    for ind, item in enumerate(df_ref[0]):\n",
    "        if tempVal == item:\n",
    "            print(item)\n",
    "            print(df_ref[1][ind])\n",
    "            tempDum[tempInd] = df_ref[1][ind]\n",
    "            \n",
    "dummy = \" \".join(tempDum)\n",
    "\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89358952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_norm(text):\n",
    "    tempText = text.split()\n",
    "    \n",
    "    for tempInd, tempVal in enumerate(tempText):\n",
    "        for ind, item in enumerate(df_ref[0]):\n",
    "            if tempVal == item:\n",
    "                tempText[tempInd] = df_ref[1][ind]\n",
    "    \n",
    "    text = \" \".join(tempText)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc78d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].map(lambda x: text_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./repo_dataset/id-multi-label-hate-speech-and-abusive-language-detection/out_clean1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36d8aa",
   "metadata": {},
   "source": [
    "Continue where i left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = './repo_dataset/id-multi-label-hate-speech-and-abusive-language-detection/out_clean1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_csv(checkpoint, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_check.drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb039e",
   "metadata": {},
   "source": [
    "kinda forget to remove numerical and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37414a0b",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea29561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "# stemming process\n",
    "sentence = 'Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan'\n",
    "output   = stemmer.stem(sentence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "# from nltk.tokenize import word_tokenize\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "kalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\n",
    "# kalimat = kalimat.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "stop = stopword.remove(kalimat)\n",
    "# tokens = nltk.tokenize.word_tokenize(stop)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd72cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "\n",
    "def stemming(text):\n",
    "\n",
    "    output = stemmer.stem(text)\n",
    "    return output\n",
    "\n",
    "def stopWordsRemoval(text):\n",
    "\n",
    "    output = stopword.remove(text)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check['Tweet'] = df_check['Tweet'].map(lambda x: stemming(x))\n",
    "df_check['Tweet'] = df_check['Tweet'].map(lambda x: stopWordsRemoval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db116cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.to_csv('./repo_dataset/id-multi-label-hate-speech-and-abusive-language-detection/out_clean2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
